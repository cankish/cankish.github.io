<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ankish Chandresh</title>
  
  <meta name="author" content="Ankish Chandresh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>


                I'm interested in machine learning, optimization, and image processing. 
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. 
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->

              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/MalleConv_after.jpg' width="160"></div>
                <img src='images/MalleConv_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>

            
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://yifanjiang.net/MalleConv.html">
                <papertitle>Fast and High-quality Image Denoising via Malleable Convolutions</papertitle>
              </a>
              <br>
              <a href="https://yifanjiang.net/">Yifan Jiang</a>,
              <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
							<a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://yifanjiang.net/MalleConv.html">project page</a>
              /
              <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
              <p></p>
              <p>
              We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
              </p>
            </td>
          </tr>
					

			    <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"  bgcolor="#ffffd0">
			      <td style="padding:20px;width:25%;vertical-align:middle">
			        <div class="one">
			          <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
			          <source src="images/refnerf.mp4" type="video/mp4">
			          Your browser does not support the video tag.
			          </video></div>
			          <img src='images/refnerf.jpg' width="160">
			        </div>
			        <script type="text/javascript">
			          function refnerf_start() {
			            document.getElementById('refnerf_image').style.opacity = "1";
			          }

			          function refnerf_stop() {
			            document.getElementById('refnerf_image').style.opacity = "0";
			          }
			          refnerf_stop()
			        </script>
			      </td>
			            <td style="padding:20px;width:75%;vertical-align:middle">
			          <a href="https://dorverbin.github.io/refnerf/index.html">
			            <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
			          </a>
			          <br>
			          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
			          <a href="https://phogzone.com/">Peter Hedman</a>,
			          <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
			          <a href="Todd Zickler">Todd Zickler</a>,
			          <strong>Jonathan T. Barron</strong>,
			          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
			          <br>
			    <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
			          <br>
			          <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
			    /
			          <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
			    /
			          <a href="https://youtu.be/qrdRH9irAlk">video</a>
			          <p></p>
			          <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
			        </td>
			      </tr>
						  														

          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfbake_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfbake_15.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfbake_160.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfbake_start() {
                  document.getElementById('nerfbake_image').style.opacity = "1";
                }

                function nerfbake_stop() {
                  document.getElementById('nerfbake_image').style.opacity = "0";
                }
                nerfbake_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://nerf.live">
              <papertitle>Baking Neural Radiance Fields for Real-Time View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www.pauldebevec.com/">Paul Debevec</a>
              <br>
							<em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://nerf.live">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.14645">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a>
              /
              <a href="https://nerf.live/#demos">demo</a>
              <p></p>
              <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>
            </td>


    
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vase_small.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall*</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>, <br>
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention, CACM Research Highlight)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&t">talk video</a>
              /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">supp video</a>
              /
              <a href="https://github.com/bmild/nerf">code</a>
              /
              <a href="https://cacm.acm.org/magazines/2022/1/257450-nerf/fulltext">CACM</a> <a href="https://cacm.acm.org/magazines/2022/1/257453-technical-perspective-neural-radiance-fields-explode-on-the-scene/fulltext">(foreward)</a>
              <p></p>
              <p>
              Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p>
            </td>
          </tr> 



 

          <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.png'></div>
                <img src='images/loss_before.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / 
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> / 
              code: <a href="https://github.com/google-research/google-research/tree/master/robust_loss">TF</a>, <a href="https://github.com/google-research/google-research/tree/master/robust_loss_jax">JAX</a>, <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/mpi_after.jpg'></div>
                <img src='images/mpi_before.jpg'>
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1TU5L6fnt4Kd49IUOU7aNxor5NIgdHuNG/view?usp=sharing">
                <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>
              </a>
              <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>, Richard Tucker,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://drive.google.com/file/d/1GUW_n-BAn9Q4VntEA_OTHNJiHO7XfC62/view?usp=sharing">supplement</a> /
              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
              <a href="data/SrinivasanCVPR2019.bib">bibtex</a>
              <p></p>
              <p>View extrapolation with multiplane images works better if you reason about disocclusions and disparity sampling frequencies.</p>
            </td>
          </tr>

          <tr onmouseout="unprocessing_stop()" onmouseover="unprocessing_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='unprocessing_image'><img src='images/unprocessing_after.jpg'></div>
                <img src='images/unprocessing_before.jpg'>
              </div>
              <script type="text/javascript">
                function unprocessing_start() {
                  document.getElementById('unprocessing_image').style.opacity = "1";
                }

                function unprocessing_stop() {
                  document.getElementById('unprocessing_image').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1H0Wtd--un2JN76dUJN8iC9fWfkA16n8D/view?usp=sharing">
                <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>
              </a>
              <br>
              <a href="http://timothybrooks.com/">Tim Brooks</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1811.11127">arxiv</a> /
              <a href="http://timothybrooks.com/tech/unprocessing/">project page</a> /
              <a href="https://github.com/google-research/google-research/tree/master/unprocessing">code</a> / 
              <a href="data/BrooksCVPR2019.bib">bibtex</a>
              <p></p>
              <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>
            </td>
          </tr>

          <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='motionblur_image'><img src='images/motionblur_after.jpg'></div>
                <img src='images/motionblur_before.jpg'>
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1hWpA4f6iLVcOkZI3zEAAWKARSQhnVgbY/view?usp=sharing">
                <papertitle>Learning to Synthesize Motion Blur</papertitle>
              </a>
              <br>
              <a href="http://timothybrooks.com/">Tim Brooks</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1811.11745">arxiv</a> /
              <a href="https://drive.google.com/file/d/1dUQwBMmQdYYIP0zHR_nDQY-uQbaMdcSN/view?usp=sharing">supplement</a> /
              <a href="http://timothybrooks.com/tech/motion-blur/">project page</a> /
              <a href="https://www.youtube.com/watch?v=8T1jjSz-2V8">video</a> /
              <a href="https://github.com/google-research/google-research/tree/master/motion_blur">code</a> / 
              <a href="data/BrooksBarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>Frame interpolation techniques can be used to train a network that directly synthesizes linear blur kernels.</p>
            </td>
          </tr>

          <tr onmouseout="darkflash_stop()" onmouseover="darkflash_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='darkflash_image'><img src='images/darkflash_after.png'></div>
                <img src='images/darkflash_before.png'>
              </div>
              <script type="text/javascript">
                function darkflash_start() {
                  document.getElementById('darkflash_image').style.opacity = "1";
                }

                function darkflash_stop() {
                  document.getElementById('darkflash_image').style.opacity = "0";
                }
                darkflash_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1901.01370">
                <papertitle>Stereoscopic Dark Flash for Low-light Photography</papertitle>
              </a>
              <br>
              <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>
              <br>
              <em>ICCP</em>, 2019
              <br>
              <p></p>
              <p>
                By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.
              </p>
            </td>
          </tr>

          <tr onmouseout="motionstereo_stop()" onmouseover="motionstereo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='motionstereo_image'><img src='images/motionstereo_after.png'></div>
                <img src='images/motionstereo_before.png'>
              </div>
              <script type="text/javascript">
                function motionstereo_start() {
                  document.getElementById('motionstereo_image').style.opacity = "1";
                }

                function motionstereo_stop() {
                  document.getElementById('motionstereo_image').style.opacity = "0";
                }
                motionstereo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing">
                <papertitle>Depth from Motion for Smartphone AR</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/valentinjulien/">Julien Valentin</a>,
              <a href="https://www.linkedin.com/in/adarshkowdle/">Adarsh Kowdle</a>,
              <strong>Jonathan T. Barron</strong>, <a href="http://nealwadhwa.com">Neal Wadhwa</a>, and others
              <br>
              <em>SIGGRAPH Asia</em>, 2018
              <br>
              <a href="https://github.com/jonbarron/planar_filter">planar filter toy code</a> / 
              <a href="data/Valentin2018.bib">bibtex</a>
              <p></p>
              <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>
            </td>
          </tr>

          <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='portrait_image'><img src='images/portrait_after.jpg'></div>
                <img src='images/portrait_before.jpg'>
              </div>
              <script type="text/javascript">
                function portrait_start() {
                  document.getElementById('portrait_image').style.opacity = "1";
                }

                function portrait_stop() {
                  document.getElementById('portrait_image').style.opacity = "0";
                }
                portrait_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing">
                <papertitle>Synthetic Depth-of-Field with a Single-Camera Mobile Phone</papertitle>
              </a>
              <br>
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,
              <a href="http://www.cs.cmu.edu/~ymovshov/">Yair Movshovitz-Attias</a>,
              <strong>Jonathan T. Barron</strong>, Yael Pritch,
              <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
              <br>
              <em>SIGGRAPH</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /
              <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">blog post</a> /
              <a href="data/Wadhwa2018.bib">bibtex</a>
              <p></p>
              <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>
              <p>This system is the basis for "Portrait Mode" on the Google Pixel 2 smartphones</p>
            </td>
          </tr>

          <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='aperture_image'><img src='images/aperture_after.jpg'></div>
                <img src='images/aperture_before.jpg'>
              </div>
              <script type="text/javascript">
                function aperture_start() {
                  document.getElementById('aperture_image').style.opacity = "1";
                }

                function aperture_stop() {
                  document.getElementById('aperture_image').style.opacity = "0";
                }
                aperture_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1MpvxcW7OTJP321QL_q4ZLQ8D653bZZzy/view?usp=sharing">
                <papertitle>Aperture Supervision for Monocular Depth Estimation</papertitle>
              </a>
              <br>
              <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2018
              <br>
              <a href="https://github.com/google/aperture_supervision">code</a> /
              <a href="data/Srinivasan2018.bib">bibtex</a>
              <p></p>
              <p>Varying a camera's aperture provides a supervisory signal that can teach a neural network to do monocular depth estimation.</p>
            </td>
          </tr>

          <tr onmouseout="deepburst_stop()" onmouseover="deepburst_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deepburst_image'><img src='images/deepburst_after.png'></div>
                <img src='images/deepburst_before.png'>
              </div>
              <script type="text/javascript">
                function deepburst_start() {
                  document.getElementById('deepburst_image').style.opacity = "1";
                }

                function deepburst_stop() {
                  document.getElementById('deepburst_image').style.opacity = "0";
                }
                deepburst_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1GAH8ijyZ7GnoBnQFANEzdXinHrE4vvXn/view?usp=sharing">
                <papertitle>Burst Denoising with Kernel Prediction Networks</papertitle>
              </a>
              <br>
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>, Robert Carroll
              <br>
              <em>CVPR</em>, 2018 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://drive.google.com/file/d/1aqk3Q-L2spjLZh2yRWKUWIDcZkGjQ7US/view?usp=sharing">supplement</a> /
              <a href="https://github.com/google/burst-denoising">code</a> /
              <a href="data/Mildenhall2018.bib">bibtex</a>
              <p></p>
              <p>We train a network to predict linear kernels that denoise noisy bursts from cellphone cameras.</p>
            </td>
          </tr>

          <tr onmouseout="friendly_stop()" onmouseover="friendly_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='friendly_image'><img src='images/friendly_after.png'></div>
                <img src='images/friendly_before.png'>
              </div>
              <script type="text/javascript">
                function friendly_start() {
                  document.getElementById('friendly_image').style.opacity = "1";
                }

                function friendly_stop() {
                  document.getElementById('friendly_image').style.opacity = "0";
                }
                friendly_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing">
                <papertitle>A Hardware-Friendly Bilateral Solver for Real-Time Virtual Reality Video</papertitle>
              </a>
              <br>
              <a href="https://homes.cs.washington.edu/~amrita/">Amrita Mazumdar</a>, <a href="http://homes.cs.washington.edu/~armin/">Armin Alaghi</a>, <strong>Jonathan T. Barron</strong>, <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>, <a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze</a>, <a href="https://homes.cs.washington.edu/~oskin/">Mark Oskin</a>, <a href="http://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
              <br>
              <em>High-Performance Graphics (HPG)</em>, 2017
              <br>
              <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a>
              <p></p>
              <p>A reformulation of the bilateral solver can be implemented efficiently on GPUs and FPGAs.</p>
            </td>
          </tr>

 

   
          <tr onmouseout="rgbd_stop()" onmouseover="rgbd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div id='rgbd_anim' class='hidden'><img src="images/SceneSIRFS.gif"></div>
              <div id='rgbd_still'><img src="images/SceneSIRFS-still.jpg"></div>
              <script type="text/javascript">
                function rgbd_start() {
                  document.getElementById('rgbd_anim').style.display = 'inline';
                  document.getElementById('rgbd_still').style.display = 'none';
                }

                function rgbd_stop() {
                  document.getElementById('rgbd_anim').style.display = 'none';
                  document.getElementById('rgbd_still').style.display = 'inline';
                }
                rgbd_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1snypSLhzC0jXCchJRsWpcDZ7Es5hDmXo/view?usp=sharing">
                <papertitle>Intrinsic Scene Properties from a Single RGB-D Image</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>CVPR</em>, 2013 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://drive.google.com/file/d/1cLUw72WpgdZ_3TQAjJABdgywqjBfn_Mq/view?usp=sharing">supplement</a> / <a href="data/BarronMalikCVPR2013.bib">bibtex</a> / <a href="http://techtalks.tv/talks/intrinsic-scene-properties-from-a-single-rgb-d-image/58614/">talk</a> / <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmWW1CZGJPbi12R0k/view?usp=sharing">keynote</a> (or <a href="https://drive.google.com/file/d/19q3EFf6GIb4UFcCN2DVU2jVKpxRj5kxf/view?usp=sharing">powerpoint</a>, <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmMzQ4ZVp1SWdnVkk/view?usp=sharing">PDF</a>) / <a href="https://drive.google.com/open?id=1ZbPScVA6Efqd-ESvojl92sw8K-82Xxry">code &amp; data</a>
              <p>By embedding mixtures of shapes &amp; lights into a soft segmentation of an image, and by leveraging the output of the Kinect, we can extend SIRFS to scenes.
                <br>
                <br>TPAMI Journal version: <a href="https://drive.google.com/file/d/1iQiUxZvjPPnb8rFCwXYesTgFSRk7mkAq/view?usp=sharing">version</a> / <a href="data/BarronMalikTPAMI2015B.bib">bibtex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Boundary.jpg" alt="Boundary_png" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1H4YPovfrvcce3HGMEhidwU2l2fTcNR5y/view?usp=sharing">
                <papertitle>Boundary Cues for 3D Object Shape Recovery</papertitle>
              </a>
              <br>
              <a href="http://www.kevinkarsch.com/">Kevin Karsch</a>,
              <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
              <a href="http://web.engr.illinois.edu/~jjrock2/">Jason Rock</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://www.cs.illinois.edu/homes/dhoiem/">Derek Hoiem</a>
              <br>
              <em>CVPR</em>, 2013
              <br>
              <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing">supplement</a> / <a href="data/KarschCVPR2013.bib">bibtex</a>
              <p>Boundary cues (like occlusions and folds) can be used for shape reconstruction, which improves object recognition for humans and computers.</p>
            </td>
          </tr>

   

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bd_promo.jpg" alt="blind-date" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                <papertitle>Blind Date: Using Proper Motions to Determine the Ages of Historical Images</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              <br>
              <em>The Astronomical Journal</em>, 136, 2008
              <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              <br>
              <em>The Astronomical Journal</em>, 135, 2008
              <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
              <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading> Misc </heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br>
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
              <br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
					

          <tr>
            <td align="center" style="padding:20px;width:25%;vertical-align:middle">
							<heading>Basically <br> Blog Posts</heading>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
              <br>
              <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
              <br>
              <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
            </td>
          </tr>
					
        </tbody></table>
 
      </td>
    </tr>
  </table>
</body>

</html>
